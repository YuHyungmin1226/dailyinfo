import requests
from bs4 import BeautifulSoup
import re

def debug_aladin_bestseller():
    """ì•Œë¼ë”˜ ì‹¤ì œ ë² ìŠ¤íŠ¸ì…€ëŸ¬ ë°ì´í„° ì°¾ê¸°"""
    url = "https://www.aladin.co.kr/shop/common/wbest.aspx?BestType=NowBest&BranchType=2&CID=0&page=1&cnt=100&SortOrder=1"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    try:
        print("ğŸ” ì•Œë¼ë”˜ ì‹¤ì œ ë² ìŠ¤íŠ¸ì…€ëŸ¬ ë°ì´í„° ì°¾ê¸°...")
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # 1. í…Œì´ë¸”ì—ì„œ ë² ìŠ¤íŠ¸ì…€ëŸ¬ ë°ì´í„° ì°¾ê¸°
        print("\nğŸ“Š í…Œì´ë¸”ì—ì„œ ë² ìŠ¤íŠ¸ì…€ëŸ¬ ë°ì´í„° ì°¾ê¸°:")
        tables = soup.find_all('table')
        
        for i, table in enumerate(tables):
            table_text = table.get_text(strip=True)
            
            # ë² ìŠ¤íŠ¸ì…€ëŸ¬ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆëŠ” í…Œì´ë¸” ì°¾ê¸°
            if any(keyword in table_text for keyword in ['ë² ìŠ¤íŠ¸', 'ìˆœìœ„', '1ìœ„', '2ìœ„', '3ìœ„']):
                print(f"\ní…Œì´ë¸” {i+1} (ë² ìŠ¤íŠ¸ì…€ëŸ¬ ê´€ë ¨):")
                print(f"  í´ë˜ìŠ¤: {table.get('class', 'í´ë˜ìŠ¤ ì—†ìŒ')}")
                print(f"  ID: {table.get('id', 'ID ì—†ìŒ')}")
                
                # í…Œì´ë¸” í–‰ ë¶„ì„
                rows = table.find_all('tr')
                print(f"  í–‰ ê°œìˆ˜: {len(rows)}")
                
                # ì²˜ìŒ ëª‡ ê°œ í–‰ì˜ ë‚´ìš© í™•ì¸
                for j, row in enumerate(rows[:5]):
                    row_text = row.get_text(strip=True)
                    if len(row_text) > 10:  # ì˜ë¯¸ìˆëŠ” ë‚´ìš©ì´ ìˆëŠ” í–‰ë§Œ
                        print(f"    í–‰ {j+1}: {row_text[:100]}...")
        
        # 2. íŠ¹ì • í´ë˜ìŠ¤ëª…ìœ¼ë¡œ ë² ìŠ¤íŠ¸ì…€ëŸ¬ ìš”ì†Œ ì°¾ê¸°
        print("\nğŸ” ë² ìŠ¤íŠ¸ì…€ëŸ¬ ê´€ë ¨ í´ë˜ìŠ¤ëª…ìœ¼ë¡œ ê²€ìƒ‰:")
        bestseller_classes = ['bestseller', 'rank', 'book_list', 'item_list', 'product']
        
        for class_name in bestseller_classes:
            elements = soup.find_all(class_=lambda x: x and class_name in str(x).lower())
            if elements:
                print(f"\n'{class_name}' í¬í•¨ í´ë˜ìŠ¤: {len(elements)}ê°œ")
                for elem in elements[:2]:
                    elem_text = elem.get_text(strip=True)
                    if len(elem_text) > 20:
                        print(f"  - {elem.name}: {elem.get('class')}")
                        print(f"    í…ìŠ¤íŠ¸: {elem_text[:100]}...")
        
        # 3. ë§í¬ì—ì„œ ë² ìŠ¤íŠ¸ì…€ëŸ¬ ë°ì´í„° ì°¾ê¸°
        print("\nğŸ”— ë§í¬ì—ì„œ ë² ìŠ¤íŠ¸ì…€ëŸ¬ ë°ì´í„° ì°¾ê¸°:")
        links = soup.find_all('a', href=True)
        book_links = [link for link in links if 'wproduct.aspx' in link.get('href', '')]
        
        print(f"ì±… ìƒí’ˆ ë§í¬: {len(book_links)}ê°œ")
        if book_links:
            for i, link in enumerate(book_links[:5]):
                link_text = link.get_text(strip=True)
                if len(link_text) > 5:
                    print(f"  {i+1}: {link_text[:50]}...")
                    print(f"    href: {link.get('href')}")
        
        # 4. ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ìˆœìœ„ íŒ¨í„´ ì°¾ê¸°
        print("\nğŸ” ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ìˆœìœ„ íŒ¨í„´ ì°¾ê¸°:")
        all_text = soup.get_text()
        
        # ë‹¤ì–‘í•œ ìˆœìœ„ íŒ¨í„´ ì‹œë„
        patterns = [
            r'(\d+)ìœ„\s*([ê°€-í£\s]+?)(?:\s+ì§€ìŒ|\s+ì €|\s+í¸ì§‘|\s+ê¸€|\s+ê·¸ë¦¼|\s+ì™¸|\s*$)',
            r'(\d+)\.\s*([ê°€-í£\s]+?)(?:\s+ì§€ìŒ|\s+ì €|\s+í¸ì§‘|\s+ê¸€|\s+ê·¸ë¦¼|\s+ì™¸|\s*$)',
            r'ìˆœìœ„\s*(\d+)[^ê°€-í£]*([ê°€-í£\s]+?)(?:\s+ì§€ìŒ|\s+ì €|\s+í¸ì§‘|\s+ê¸€|\s+ê·¸ë¦¼|\s+ì™¸|\s*$)',
        ]
        
        for i, pattern in enumerate(patterns):
            matches = re.findall(pattern, all_text)
            if matches:
                print(f"\níŒ¨í„´ {i+1} ë§¤ì¹˜: {len(matches)}ê°œ")
                for match in matches[:5]:
                    rank = match[0]
                    title = match[1].strip()
                    if len(title) > 5:
                        print(f"  - {rank}ìœ„: {title[:40]}...")
        
        # 5. íŠ¹ì • IDë‚˜ í´ë˜ìŠ¤ë¡œ ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì˜ì—­ ì°¾ê¸°
        print("\nğŸ¯ íŠ¹ì • ì˜ì—­ì—ì„œ ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì°¾ê¸°:")
        specific_selectors = [
            '#bestseller',
            '.bestseller',
            '#book_list',
            '.book_list',
            '#rank_list',
            '.rank_list',
            '#product_list',
            '.product_list'
        ]
        
        for selector in specific_selectors:
            elements = soup.select(selector)
            if elements:
                print(f"\n{selector}: {len(elements)}ê°œ")
                for elem in elements[:1]:
                    elem_text = elem.get_text(strip=True)
                    if len(elem_text) > 50:
                        print(f"  í…ìŠ¤íŠ¸: {elem_text[:100]}...")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    debug_aladin_bestseller() 